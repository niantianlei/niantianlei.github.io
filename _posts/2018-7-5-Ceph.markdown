---
layout:     post
title:      "Ceph学习笔记"
subtitle:   " \"Ceph learning\""

author:     "Nian Tianlei"
header-img: "img/post-bg-2016.jpg"
header-mask: 0.4
catalog:    true
tags:
    - 分布式
    - 分布式存储
---


## Ceph简介

Ceph，一个PB级别分布式存储系统，Ceph可分为三块：对象存储、块设备存储和文件系统服务。在虚拟化领域里，比较常用到的是Ceph的块设备存储。  

Ceph相比其它存储的优势点在于它不单单是存储，同时还充分利用了存储节点上的计算能力，在存储每一个数据时，都会通过计算得出该数据存储的位置，尽量将数据分布均衡，同时由于Ceph的良好设计，采用了CRUSH算法、HASH环等方法，使得它不存在传统的单点故障的问题，且随着规模的扩大性能并不会受到影响。  



## Ceph核心组件

Ceph的核心组件包括Ceph OSD、Ceph Monitor和Ceph MDS。   

Ceph 存储集群至少需要一个 Ceph Monitor 和两个 OSD 守护进程  

**Ceph OSD：**Object Storage Device，它的主要功能是存储数据、复制数据、平衡数据、恢复数据等，与其它OSD间进行心跳检查等，并将一些变化情况上报给Ceph Monitor，用作监控。一般情况下一块硬盘对应一个OSD，由OSD来对硬盘存储进行管理，当然一个分区也可以成为一个OSD。       

伴随OSD的还有一个概念叫做Journal盘，一般写数据到Ceph集群时，都是先将数据写入到Journal盘中，然后每隔一段时间比如5秒再将Journal盘中的数据刷新到文件系统中。一般为了使读写时延更小，Journal盘都是采用SSD，一般分配10G以上，当然分配多点那是更好，Ceph中引入Journal盘的概念是因为Journal允许Ceph OSD功能很快做小的写操作；一个随机写入首先写入在上一个连续类型的journal，然后刷新到文件系统，这给了文件系统足够的时间来合并写入磁盘，一般情况下使用SSD作为OSD的journal可以有效缓冲突发负载。  

**Ceph Monitor：**监视器，负责监视Ceph集群，维护Ceph集群的健康状态，同时维护着Ceph集群中的各种Map图，比如OSD Map、Monitor Map、PG Map和CRUSH Map，这些Map统称为Cluster Map，Cluster Map是RADOS的关键数据结构，管理集群中的所有成员、关系、属性等信息以及数据的分发，比如当用户需要存储数据到Ceph集群时，OSD需要先通过Monitor获取最新的Map图，然后根据Map图和object id等计算出数据最终存储的位置。  

**Ceph MDS：**Ceph MetaData Server，主要保存的文件系统服务的元数据，但对象存储和块存储设备是不需要使用该服务的。  



## Ceph基本架构
![a]({{ "/img/post/Ceph/Ceph1.gif" | prepend: site.baseurl }} )   
由图可见，Ceph系统可分为4块：客户端（数据用户），元数据服务器（缓存和同步分布式元数据），对象存储集群（将数据和元数据作为对象存储，执行其他关键职能），以及最后的集群监视器（执行监视功能）。  

 

客户使用元数据服务器，执行元数据操作（来确定数据位置）。元数据服务器管理数据位置，以及在何处存储新数据。值得注意的是，元数据存储在一个存储集群（标为 “元数据 I/O”）。实际的文件 I/O 发生在客户和对象存储集群之间。这样一来，更高层次的 POSIX 功能（例如，打开、关闭、重命名）就由元数据服务器管理，不过 POSIX 功能（例如读和写）则直接由对象存储集群管理。  

 



下图是一个简单的Ceph 生态系统。  

Ceph Client 是 Ceph 文件系统的用户。Ceph Metadata Daemon 提供了元数据服务器，而 Ceph Object Storage Daemon 提供了实际存储（对数据和元数据两者）。最后，Ceph Monitor 提供了集群管理。要注意的是，Ceph 客户，对象存储端点，元数据服务器（根据文件系统的容量）可以有许多，而且至少有一对冗余的监视器。   
![a]({{ "/img/post/Ceph/Ceph2.gif" | prepend: site.baseurl }} )   

 

**Ceph 客户端**  
因为 Linux 为文件系统提供了一个通用接口，Ceph 的用户视角是透明的。考虑到多个服务器组成存储系统的可能性，管理员视角当然会不同。从使用者的视角看，他们可以使用海量存储系统，并不知晓下层的元数据服务器、监控、聚合成大规模存储池的各个对象存储设备。使用者仅仅看到一个挂载点，从那里执行标准的文件 I/O 操作。  




Ceph 文件系统是在 Linux 内核中实现（大多数文件系统，控制与智能都实现在内核的文件系统源代码中）。但对 Ceph 而言，文件系统的智能分布在各节点上，它简化了客户端接口，但也同时提供给 Ceph 应对大规模（甚至动态的）数据的能力。  

并非是依赖于分配链（用来映射磁盘块到指定文件的元数据），Ceph 使用了一个有趣的选择。一个文件从 Linux 的角度看，被赋予了一个来自元数据服务器的 inode 号（INO），它是文件的唯一识别符。之后这个文件被刻进到多个对象中（基于文件大小）。使用 INO 和 对象号 （ONO），每个对象被赋予了一个对象 ID（OID）。通过使用 OID 上的一个简单哈希，每个对象被分配到 一个放置组中。这个放置组（通过 PGID 来识别）是对象的概念上的容器。最后，放置组到对象存储设备的映射是一种使用了称为可扩展哈希受控复制（CRUSH）算法的伪随机映射。以这种方式，放置组（及副本）映射到存储设备不依赖于任何元数据，而是依赖于伪随机映射函数。这种行为是理想的，因为它最小化了存储的负荷，以及简化了数据的分布和查询。  

最后一个分配组件是集群映射。集群映射是代表着存储集群的设备的有效表示。通过 PGID 和 集群映射，你可以定位任何对象。  

 

**Ceph 元数据服务器**  
元数据服务器的任务 (命令) 是管理文件系统的命名空间。尽管不管是数据还是元数据都存储在对象存储集群中，它们都被分别管理，以支持可扩展性。事实上，元数据在元数据服务器的集群间进一步的分割，这些元数据服务器能够自适应的复制和散布命名空间，从而避免热点。元数据服务器管理部分命名空间，并能部分重叠（出于冗余及性能的考虑）。从元数据服务器到命名空间的映射，在 Ceph 中通过动态子树分区的方式来执行，它允许 Ceph 在保留本地性能的同时可以适应变化的工作负荷（在元数据服务器间迁移命名空间）。  



但因为每个元数据服务器就客户端的个数而言只简单的管理命名空间，它主要的应用程序是智能元数据缓冲（因为实际的元数据最终存储在对象存储集群中）。写入的元数据被缓冲在短期的日志中，它最终会被推送到物理存储里。这种行为允许元数据服务器可以立即对客户端提供最新的元数据服务（这在元数据操作中很常见）。日志对故障恢复也很有用：如果元数据服务器失效，它的日志能被重播，以确保元数据安全的保存到磁盘上。  



**Ceph 监控**  
Ceph 包括了实现集群映射管理功能的监控，但一些容错管理的要素实现于对象存储本身。当对象存储设备失效，或者新的设备被添加，监控检测并维持一个有效的集群映射。这个功能以分布式的方式执行，在这里，更新映射会与当前的流量状况进行沟通。Ceph 使用了 Paxos，它是分布式一致性的一系列算法。  

**Ceph 对象存储**  
与传统对象存储相似，Ceph 存储节点不仅包含了存储，也包含了智能。传统的驱动器是的简单目标端，仅仅响应发起端的命令。而对象存储设备是智能设备，既可以作为目标端，也可以作为发起端，从而支持通讯以及其它对象存储设备的协同。   



从存储的角度看，Ceph对象存储提供了从“对象”到“块”的映射（传统上这个事情在客户端的文件系统层来做）。这个行为允许本地实体对如何来存储对象做出最好的选择。早期的Ceph版本使用了一个在本地文件系统上的自定义的底层文件系统（EBOFS）。这个系统实现了一个非标准接口的底层存储来提供对象语义和其他特征（如对磁盘提交的异步通知）。现在可以在存储节点上使用B-tree文件系统（BTRFS），它已经实现了一些必要的功能（如嵌入式完整性）。因为Ceph客户端使用CRUSH，而不知道磁盘上文件的块映射信息， 所以底层的存储设备可以安全的管理从对象到块的映射。 这使得，当设备被发现失效后，依然节点复制数据。失效恢复的分布式机制也提升了存储系统的扩展性，因为失效检测和恢复在整个生态系统得到分布。Ceph中称之为RADOS（见上图）。



## Ceph数据分布算法

数据分布均衡是分布式存储系统中较为重要的一点，目的就是提高资源利用率，最大化系统性能，也便于维护。常见的数据分布算法有一致性Hash和Ceph的Crush算法。Crush是一种伪随机的控制数据分布、复制的算法，Ceph是为大规模分布式存储而设计的，数据分布算法必须能够满足在大规模的集群下数据依然能够快速的准确的计算存放位置，同时能够在硬件故障或扩展硬件设备时做到尽可能小的数据迁移，Ceph的CRUSH算法就是精心为这些特性设计的，可以说CRUSH算法也是Ceph的核心之一。  



存储数据与object：当用户要将数据存储到Ceph集群时，存储数据都会被分割成多个object，每个object都有一个object id，每个object的大小是可以设置的，默认是4MB，object可以看成是Ceph存储的最小存储单元。  

object与pg的关系：由于object的数量很多，所以Ceph引入了pg的概念用于管理object，每个object最后都会通过CRUSH计算映射到某个pg中，通常情况下一个pg包含多个object。  

pg与osd的关系：pg也需要通过CRUSH计算映射到osd中去存储，如果是二副本的，则每个pg都会映射到二个osd，比如[osd.1,osd.2]，那么osd.1是存放该pg的主副本，osd.2是存放该pg的从副本，保证了数据的冗余。  

pg和pgp的关系：pg是用来存放object的，pgp相当于是pg存放osd的一种排列组合，我举个例子，比如有3个osd，osd.1、osd.2和osd.3，副本数是2，如果pgp的数目为1，那么pg存放的osd组合就只有一种，可能是[osd.1,osd.2]，那么所有的pg主从副本分别存放到osd.1和osd.2，如果pgp设为2，那么其osd组合可以两种，可能是[osd.1,osd.2]和[osd.1,osd.3]，是不是很像我们高中数学学过的排列组合，pgp就是代表这个意思。一般来说应该将pg和pgp的数量设置为相等。   



本质上CRUSH算法是根据存储设备的权重来计算数据对象的分布的，权重的设计可以根据该磁盘的容量和读写速度来设置，比如根据容量大小可以将1T的硬盘设备权重设为1，2T的就设为2，在计算过程中，CRUSH是根据Cluster Map、数据分布策略和一个随机数共同决定数组最终的存储位置的。  

Cluster Map里的内容信息包括存储集群中可用的存储资源及其相互之间的空间层次关系，比如集群中有多少个支架，每个支架中有多少个服务器，每个服务器有多少块磁盘用以OSD等。  

数据分布策略是指可以通过Ceph管理者通过配置信息指定数据分布的一些特点，比如管理者配置的故障域是Host，也就意味着当有一台Host起不来时，数据能够不丢失，CRUSH可以通过将每个pg的主从副本分别存放在不同Host的OSD上即可达到，不单单可以指定Host，还可以指定机架等故障域，除了故障域，还有选择数据冗余的方式，比如副本数或纠删码。  

下面这个式子简单的表明CRUSH的计算表达式：  
CRUSH(X)  -> (osd.1,osd.2.....osd.n)  
式子中的X就是一个随机数。  

下面通过一个计算PG ID的示例来看CRUSH的一个计算过程：  
（1）Client输入Pool ID和对象ID；  
（2）CRUSH获得对象ID并对其进行Hash运算；  
（3）CRUSH计算OSD的个数，Hash取模获得PG的ID，比如0x48；  
（4）CRUSH取得该Pool的ID，比如是1；  
（5）CRUSH预先考虑到Pool ID相同的PG ID，比如1.48。  


## Ceph IO流程
#### 正常IO流程图
![IO]({{ "/img/post/Ceph/IO1.png" | prepend: site.baseurl }} )   
步骤：  
1. client 创建cluster handler。  
2. client 读取配置文件。  
3. client 连接上monitor，获取集群map信息。  
4. client 读写io 根据crshmap 算法请求对应的主osd数据节点。  
5. 主osd数据节点同时写入另外两个副本节点数据。  
6. 等待主节点以及另外两个副本节点写完数据状态。  
7. 主节点及副本节点写入状态都成功后，返回给client，io写入完成。  

#### 新主IO流程图
说明：  
如果新加入的OSD1取代了原有的 OSD4成为 Primary OSD, 由于 OSD1 上未创建 PG , 不存在数据，那么 PG 上的 I/O 无法进行，怎样工作的呢？  
![IO]({{ "/img/post/Ceph/IO2.png" | prepend: site.baseurl }} )   
步骤：  
1. client连接monitor获取集群map信息。  
2. 同时新主osd1由于没有pg数据会主动上报monitor告知让osd2临时接替为主。  
3. 临时主osd2会把数据全量同步给新主osd1。  
4. client IO读写直接连接临时主osd2进行读写。  
5. osd2收到读写io，同时写入另外两副本节点。  
6. 等待osd2以及另外两副本写入成功。  
7. osd2三份数据都写入成功返回给client, 此时client io读写完毕。  
8. 如果osd1数据同步完毕，临时主osd2会交出主角色。  
9. osd1成为主节点，osd2变成副本。  


#### Ceph IO算法流程（伪代码）
```
locator = object_name
obj_hash =  hash(locator)
pg = obj_hash % num_pg
osds_for_pg = crush(pg)    # returns a list of osds
primary = osds_for_pg[0]
replicas = osds_for_pg[1:]
```
#### Ceph RBD IO流程
数据组织  
![IO]({{ "/img/post/Ceph/organize.png" | prepend: site.baseurl }} )   
步骤：  
1.  客户端创建一个pool，需要为这个pool指定pg的数量。  
2.  创建pool/image rbd设备进行挂载。  
3.  用户写入的数据进行切块，每个块的大小默认为4M，并且每个块都有一个名字，名字就是object+序号。  
4.  将每个object通过pg进行副本位置的分配。  
5.  pg根据cursh算法会寻找3个osd，把这个object分别保存在这三个osd上。  
6.  osd上实际是把底层的disk进行了格式化操作，一般部署工具会将它格式化为xfs文件系统。  
7.  object的存储就变成了存储一个文rbd0.object1.file。  

## Ceph通信
#### 网络通信框架三种不同的实现方式  
1.Simple线程模式  
特点：每一个网络链接，都会创建两个线程，一个用于接收，一个用于发送。  
缺点：大量的链接会产生大量的线程，会消耗CPU资源，影响性能。  

2.Async事件的I/O多路复用模式  
特点：这种是目前网络通信中广泛采用的方式。k版默认已经使用Asnyc了。  

3.XIO方式使用了开源的网络通信库accelio来实现  
特点：这种方式需要依赖第三方的库accelio稳定性，目前处于试验阶段。  

#### Ceph通信框架设计模式
设计模式(Subscribe/Publish)：  
订阅发布模式又名观察者模式，它意图是“定义对象间的一种一对多的依赖关系，  
当一个对象的状态发生改变时，所有依赖于它的对象都得到通知并被自动更新”。  

#### Ceph通信框架流程图
![info]({{ "/img/post/Ceph/info.png" | prepend: site.baseurl }} )   
步骤：  
•  Accepter监听peer的请求, 调用 SimpleMessenger::add_accept_pipe() 创建新的 Pipe 到 SimpleMessenger::pipes 来处理该请求。

•  Pipe用于消息的读取和发送。该类主要有两个组件，Pipe::Reader，Pipe::Writer用来处理消息读取和发送。

•  Messenger作为消息的发布者, 各个 Dispatcher 子类作为消息的订阅者, Messenger 收到消息之后，  通过 Pipe 读取消息，然后转给 Dispatcher 处理。

•  Dispatcher是订阅者的基类，具体的订阅后端继承该类,初始化的时候通过 Messenger::add_dispatcher_tail/head 注册到 Messenger::dispatchers. 收到消息后，通知该类处理。

•  DispatchQueue该类用来缓存收到的消息, 然后唤醒 DispatchQueue::dispatch_thread 线程找到后端的 Dispatch 处理消息。
